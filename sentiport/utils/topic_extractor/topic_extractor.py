"""Keyword Extractor - Unit 1 Version 1.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1I0kXU1U3SVsg3jNeSyVQpNLrirsks4sw?usp=sharing
"""

#@title Preparing Environment
# NLU preparation

import pandas as pd
import numpy as np
import nlu
import re
import string
import nltk
from nltk.stem import WordNetLemmatizer 
from nltk.tokenize import sent_tokenize

from google_play_scraper import app, reviews, reviews_all, Sort
from tqdm import tqdm 
import spacy
from textblob import TextBlob
from reportlab.platypus import Table
from reportlab.platypus import TableStyle
from reportlab.lib import colors
from reportlab.platypus import Paragraph
from reportlab.lib.styles import ParagraphStyle
from tqdm import tqdm 
from langdetect import detect
import pandas as pd
import math
import re

lemmatizer = WordNetLemmatizer() 
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
pair_dict = [[['JJ'],['NN']]]
pos_dict = ['NN']

english_stopword = ['pls','app','much','very','really','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', "i'll", "you'll", "he'll", "she'll", "we'll", "they'll", "i'd", "you'd", "he'd", "she'd", "we'd", "they'd", "i'm", "you're", "he's", "she's", "it's", "we're", "they're", "i've", "we've", "you've", "they've", "isn't", "aren't", "wasn't", "weren't", "haven't", "hasn't", "hadn't", "don't", "doesn't", "didn't", "won't", "wouldn't", "shan't", "shouldn't", "mustn't", "can't", "couldn't", 'cannot', 'could', "here's", "how's", "let's", 'ought', "that's", "there's", "what's", "when's", "where's", "who's", "why's", 'would', 'not', 'though', 'altough']
bahasa_stopwords = ['nya', 'ga','gak','bs', 'paling', 'tentu', 'seraya', 'mengucapkannya', 'sepantasnya', 'sebegini', 'kelima', 'sabtu', 'belum', 'agaknya', 'rt', 'masalah', 'dipersoalkan', 'meningkatkan', 'sempat', 'jadinya', 'karena', 'diberikannya', 'siapa', 'dulu', 'sesuatu', 'jadi', 'dimungkinkan', 'suatu', 'di', 'contohnya', 'tadinya', 'sekedar', 'pukul', 'kiranya', 'kemungkinan', 'penting', 'antara', 'wib ', 'semisal', 'apalagi', 'adalah', 'tanya', 'tadi', 'mengatakan', 'seingat', 'kita', 'kira-kira', 'bakal', 'dini', 'warga ', 'lama', 'beliau', 'gimana', 'lima', 'seketika', 'sebutlah', 'ribu', 'sebegitu', 'sekadarnya', 'sejenak', 'hidup', 'awalnya', 'segalanya', 'sedikit', 'terus', 'terlihat', 'diperbuatnya', 'berlangsung', 'sepanjang', 'yg', 'lu', 'ditanyai', 'kapanpun', 'ungkapnya', 'benarkah', 'makin', 'sebelumnya', 'nanti', 'pernah', 'beginilah', 'katanya', 'tentang', 'tambah', 'baru', 'sama-sama', 'kamilah', 'didatangkan', 'ke', 'ditunjuk', 'sebenarnya', 'penggunaan', 'seseorang', 'kembali', 'cukuplah', 'tidakkah', 'terdiri', 'wakil ', 'jangankan', 'harusnya', 'malah', 'sekitar', 'kalaupun', 'yang', 'telah', 'seluruhnya', 'terakhir', 'kegiatan', 'kalau', 'mengambil', 'sehingga', 'apa', 'boleh', 'melainkan', 'bermacam', 'diberi', 'sejauh', 'toh', 'sekalian', 'umum', 'menyebutkan', 'oleh', 'menunjuk', 'sebelum', 'perlukah', 'masih', 'semampunya', 'sesudahnya', 'kami', 'dsb', 'seringnya', 'sebaik-baiknya', 'semula', 'untuk', 'termasuk', 'minggu', 'diucapkannya', 'ibaratnya', 'tentulah', 'mengibaratkannya', 'sebut', 'bulan', 'dipunyai', 'tertuju', 'terjadilah', 'ditandaskan', 'sekurang-kurangnya', 'mendatangkan', 'menjalani', 'berkata', 'dikatakannya', 'sesekali', 'bukan', 'disebut', 'kondisi', 'terbanyak', 'bapak', 'tetap', 'sewaktu', 'mempergunakan', 'begitu', 'tersebutlah', 'tiap', 'tampaknya', 'mengapa', 'sebagainya', 'inginkan', 'pekan', 'dituturkannya', 'lamanya', 'pihaknya', 'hanya', 'ingat-ingat', 'lebih', 'serupa', 'sampai', 'tepat', 'selain', 'dari', 'gitu', 'mendapat', 'sekurangnya', 'lanjut', 'beginikah', 'ujarnya', 'perlunya', 'dahulu', 'datang', 'naik', 'tampak', 'dia', 'ketiga', 'dijelaskan', 'terjadi', 'memperlihatkan', 'cuma', 'luar', 'dua', 'pak', 'tolong', 'berturut', 'per', 'langsung', 'padahal', 'sesaat', 'sebesar', 'apakah', 'bisa', 'segera', 'nggak', 'dibuatnya', 'wong', 'waktu', 'semampu', 'menyampaikan', 'katakanlah', 'walau', 'bila', 'sedang', 'diperlukan', 'kelihatan', 'se', 'pantas', 'the', 'bertemu', 'tuturnya', 'menantikan', 'masa', 'kamu', 'nyaris', 'sebetulnya', 'memastikan', 'bagi', 'akan', 'bagaimana', 'sebagian', 'dimana', 'lah', 'dekat', 'segala', 'olehnya', 'memungkinkan', 'sekali', 'jangan', 'makanya', 'akhirnya', 'asal', 'mengingatkan', 'mungkinlah', 'dulunya', 'mengenai', 'dan', 'seluruh', 'andalah', 'kali', 'hendaknya', 'semata-mata', 'menambahkan', 'diakhirinya', 'pun', 'pihak', 'seorang', 'bertutur', 'mungkinkah', 'saya', 'atau', 'kalaulah', 'dirinya', 'pertanyaan', 'sedangkan', 'saja', 'kok', 'sekecil', 'tutur', 'namun', 'hendaklah', 'merekalah', 'manakala', 'selanjutnya', 'sementara', 'ataupun', 'secara', 'ucapnya', 'haruslah', 'dimaksud', 'begini', 'berikut', 'siapakah', 'baik', 'terjadinya', 'jumlah', 'beri', 'sekarang', 'masing', 'adanya', 'sama', 'lagi', 'upaya', 'setempat', 'sini', 'bermaksud', 'dialah', 'ataukah', 'serta', 'hendak', 'bagaimanapun', 'sedikitnya', 'mempunyai', 'jawaban', 'jelaslah', 'saling', 'pasti', 'setelah', 'itulah', 'enggak', 'sekadar', 'lain', 'percuma', 'sudahlah', 'nyatanya', 'menunjukkan', 'menyeluruh', 'melalui', 'selasa', 'seolah', 'biasa', 'manalagi', 'mereka', 'mau', 'yaitu', 'dengan', 'membawa', 'kinilah', 'tersampaikan', 'demikianlah', 'diminta', 'pada', 'juni', 'orang', 'sesuatunya', 'seusai', 'besar', 'misalnya', 'sesegera', 'misal', 'nah', 'hanyalah', 'saatnya', 'merupakan', 'amatlah', 'akhir', 'begitulah', 'banyak', 'keseluruhan', 'kapan', 'melihatnya', 'daripada', 'siap', 'artinya', 'setibanya', 'jika', 'tetapi', 'seenaknya', 'bolehlah', 'berlainan', 'persoalan', 'kah', 'bertanya', 'setiba', 'sepertinya', 'pentingnya', 'juga', 'kurang', 'seperlunya', 'punya', 'dimaksudkannya', 'soalnya', 'belumlah', 'tinggi', 'milik', 'usai', 'ingin', 'persen', 'selalu', 'diinginkan', 'tiga', 'lainnya', 'sekiranya', 'jenis', 'mempersoalkan', 'disebutkannya', 'terlalu', 'menghadapi', 'diperkirakan', 'tertentu', 'panjang', 'demi', 'guna', 'setengah', 'sendirian', 'semaunya', 'apabila', 'sambil', 'tersebut', 'saat', 'depan', 'mampu', 'setinggi', 'merasa', 'keterlaluan', 'memisalkan', 'walaupun', 'waktunya ', 'satu', 'bilakah', 'bahwasanya', 'dikira', 'terdahulu', 'tegasnya', 'bawah', 'berakhirlah', 'waduh', 'betul', 'semakin', 'semata', 'kenapa', 'ditemukan', 'tempat', 'kapankah', 'sekaligus', 'tandasnya', 'bekerja', 'ikut', 'macam', 'menanyai', 'semacam', 'meminta', 'dikarenakan', 'pula', 'paparnya', 'sebaik', 'keinginan', 'tiba-tiba', 'sebaliknya', 'bahwa', 'sayalah', 'terkira', 'wahai', 'pastilah', 'ujar', 'no', 'aku', 'mendatang', 'sendiri', 'ternyata', 'akulah', 'ibarat', 'telanjur', 'sejak', 'meski', 'kecuali', 'bersama', 'nantinya', 'mencari', 'mengakhiri', 'tanyakan', 'akankah', 'seperti', 'para', 'sejumlah', 'selama', 'berkeinginan', 'membantu', 'semua', 'kamulah', 'bukankah', 'rasa', 'seterusnya', 'lo', 'diingat', 'mana', 'perlu', 'sebabnya', 'tahun', 'terhadap', 'seberapa', 'ucap', 'bersiap', 'berjumlah', 'belakangan', 'sajalah', 'harus', 'tahu', 'berat', 'oh', 'dapat', 'dimulailah', 'sebanyak', 'balik', 'dilakukan', 'kalian', 'hubungan', 'kemana', 'kedua', 'bagaikan', 'kemarin', 'meyakini', 'yakin', 'tengah', 'tegas', 'sebisanya', 'semasa', 'tentunya', 'terasa', 'kepadanya', 'sudahkah', 'sampaikan', 'pertanyakan', 'amat', 'beberapa', 'pertama-tama', 'rasanya', '&', 'tampil', 'ini', 'itukah', 'tanyanya', 'teringat', 'inginkah', 'bahkan', 'usah', 'emang', 'selamanya', 'ketika', 'sedemikian', 'dong', 'terhadapnya', 'senin', 'kehidupan', 'sampai-sampai', 'berarti', 'terlebih', 'diibaratkan', 'khususnya', 'diketahui', 'menjadi', 'cukup', 'berupa', 'sela', 'tidak', 'wib', 'dipastikan', 'justru', 'setidaknya', 'ditunjukkan', 'terdapat', 'wah', 'sebagaimana', 'pertama', 'entah', 'waktunya', 'sumber', 'tidaklah', 'anda', 'sendirinya', 'hal', 'menyatakan', 'sesudah', 'adapun', 'sebab', 'sangat', 'berbagai', 'menjelaskan', 'sana', 'ungkap', 'tandas', 'sebagai', 'dimisalkan', 'sinilah', 'sekian', 'menanti', 'sekali-kali', 'tingkat', 'kebetulan', 'karenanya', 'diantara', 'jumlahnya', 'semisalnya', 'sebuah', 'bukanlah', 'tambahnya', 'berdasarkan', 'kala', 'terutama', 'setidak-tidaknya', 'mengira', 'disinilah', 'dll', 'berapalah', 'agar', 'selama-lamanya', 'sebutnya', 'teringat-ingat', 'buat', 'ada', 'itu', 'selaku', 'rata', 'siapapun', 'jelas', 'selagi', 'sesuai', 'jauh', 'sangatlah', 'sesampai', 'asalkan', 'memberi', 'mulai', 'semasih', 'seolah-olah', 'kemudian', 'menurut', 'dilalui', 'maupun', 'minta', 'ya', 'lalu', 'antar', 'setiap', 'sudah', 'rupanya', 'sekalipun', 'berapa', 'acara', 'cara', 'hari', 'sepantasnyalah', 'demikian', 'sepihak', 'kata', 'mantan', 'supaya', 'tanpa', 'ia', 'ditegaskan', 'mengetahui', 'padanya', 'bagaimanakah', 'tak', 'mulanya', 'seharusnya', 'umumnya', 'ditunjuknya', 'secukupnya', 'tinggal', 'kepada', 'atas', 'khusus', 'sesama', 'turut', 'soal', 'tiba', 'biasanya', 'berada', 'tapi', 'semuanya', 'yakni', 'memerlukan', 'menghendaki', 'sebaiknya', 'ruang', 'memberikan', 'sering', 'tunjuk', 'bagai', 'masihkah', 'diri', 'inilah', 'sekitarnya', 'lewat', 'aplikasi']
fullstop = bahasa_stopwords+english_stopword

#@title Program Calvin
#@title Default title text
def get_crawl_google (id, country_id):

  '''
  This function is used to crawl data from google playstore
  Parameters:
  id-- application's playstore id
  country_id-- application's country id
  returns:
  df_google-- crawled google dataframe (columns = 'reviewID','review','version','rating','at')
  '''

  BATCH_SIZE = 50
  MAX_REVIEWS = 5000
  appinfo = app(
      id,
      lang='id',
      country=country_id)
  
  appDesc = appinfo['description']

  appinfo['title']
  AVAIL_REVIEWS = appinfo.get('reviews')
  TOFETCH_REVIEWS = min(AVAIL_REVIEWS, MAX_REVIEWS)
  ints = list(range(TOFETCH_REVIEWS//BATCH_SIZE))
  t = tqdm(total=TOFETCH_REVIEWS)
  continuation_token = ""
  result = []

  for i in ints:
      if i == 0:
        result, continuation_token = reviews(id,
                                           count=BATCH_SIZE,
                                           country=country_id
                                           )
      res, continuation_token = reviews(id, count=BATCH_SIZE, continuation_token=continuation_token)
      result.extend(res)
      t.update(BATCH_SIZE)
  t.close()
  AVAIL_REVIEWS
  dfp = pd.DataFrame(result)
  dfp.drop_duplicates('reviewId', inplace=True) #droppping the duplicates

  # creating the dataframe
  data = [dfp['reviewId'],dfp['content'],dfp['reviewCreatedVersion'],dfp['score'],dfp['at']]
  headers = ['reviewId','review','version','rating','at']
  df_google = pd.concat(data, axis=1, keys=headers)
  df_google['version'].fillna("null",inplace=True)
  
  # fill the null value on the version
  for idx in range(len(df_google)-1):
    if df_google['version'][idx] == 'null' :
      df_google.loc[idx,'version']= df_google['version'][idx+1]
  
  # drop version which lead to error (ex: '334280')
  for i in range(len(df_google)):
    if "." in df_google['version'][i][1]:
      pass
    elif "." in df_google['version'][i][2]:
      pass
    else:  
      df_google.drop(index=i,inplace=True)
  df_google.reset_index(drop=True, inplace=True)
  df_google['at'] = pd.to_datetime(df_google['at']) #set the 'at' column as datetime
  df_google.drop_duplicates(subset ="reviewId", 
                     keep = False, inplace = True) 
  return df_google, appDesc

def app_info(id, country_id):
  appinfo = app(
    id,
    lang='en',
    country=country_id
  )
  return appinfo['title'],appinfo['description']

# @title YAKE helper functions
def concatening_ngrams(keywords):
  try:
    return re.sub(' ','_', keywords)
  except:
    return keywords

#NLTK
def nltk_lemmatize(sentence, isConcatenated):
  # print(sentence)
  if isConcatenated:
    return "_".join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(sentence))
  else:
    return " ".join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(sentence))

#Spacy
def spacy_lemmatize(sentence, isConcatenated):
    #tags=['ADJ','NOUN','']
  if isConcatenated:
    connector = '_'
  else:
    connector = ' '
  try:
    doc = nlp(sentence)
    result = connector.join([token.lemma_ for token in doc if token.lemma_ != '-PRON-'])
    return result
  except:
    if isConcatenated:
      return "_".join(sentence.split(' '))
    else:
      return sentence

def keyword_lemmatize(keyword, isConcatenated):
  if isConcatenated:
    connector = "_"
  else:
    connector = " "
  try:
    doc = nlp(keyword)
    result = connector.join([token.lemma_ for token in doc if token.lemma_ != '-PRON-'])
    if result != keyword:
      if detect(f"The word is {keyword}") != 'en':
        return connector.join(keyword.split(" "))
    else:
      return result
  except:
    if isConcatenated:
      return "_".join(keyword.split(' '))
    else:
      return keyword

#Final lemmatization
def final_lemmatize(sentence, isConcatenated):
  sentence = nltk_lemmatize(sentence, False)
  sentence = spacy_lemmatize(sentence)
  if isConcatenated:
    return "_".join([word for word in sentence.split(' ')])
  else:
    return sentence

def YAKEKeywords(listOfSentences, minNGrams, maxNGrams, nKeywords, windowSize, removeNotFromStopWords = True):
  keywordExtractor = nlu.load('yake') #loading the YAKE
  # Setting up the pipeline
  keywordExtractor['yake'].setMinNGrams(minNGrams) 
  keywordExtractor['yake'].setMaxNGrams(maxNGrams)
  keywordExtractor['yake'].setNKeywords(nKeywords)
  keywordExtractor['yake'].setWindowSize(windowSize)
  #Customizing the Stop Words used in the pipeline
  keywordExtractor['yake'].setStopWords(fullstop)
  keys_df = keywordExtractor.predict(listOfSentences, output_level = 'keywords')
  # keys_df.drop_duplicates(inplace = True)
  keys_df.dropna(subset = ['keywords'], inplace = True)
  return keys_df


# @title Hops Phrase Finder helper functions
def find_phrase(review, pair_dict = pair_dict, maxhops = 3):
  output = []
  review_sent = sent_tokenize(review)
  length = 0
  for sent in review_sent:
    tokens = nlp(sent)
    for pairs in pair_dict:
      a = pairs[0]
      b = pairs[1]
      for i in range(len(tokens)):
        if tokens[i].tag_ in a and re.findall('\W',tokens[i].text) == []:
          hop = 1
          cond = True
          while hop <= maxhops:
            if i+hop < len(tokens):
              if tokens[i+hop].tag_ in b and re.findall('\W',tokens[i+hop].text) == [] and tokens[i+hop].text != 'app':
                bigram = "_".join([tokens[i].text, tokens[i+hop].text])
                hop = maxhops
                output.append(bigram)
                cond = False
            if i-hop > -1 and cond:
              if tokens[i-hop].tag_ in b and re.findall('\W',tokens[i-hop].text) == [] and tokens[i-hop].text != 'app':
                bigram = "_".join([tokens[i].text, tokens[i-hop].text])
                hop = maxhops
                output.append(bigram)
            hop+=1
    length += len(tokens)
  return output

def find_pos_pairs(review, pos_dict = pos_dict, maxhops = 3):
  output = []
  review_sent = sent_tokenize(review)
  length = 0
  for sent in review_sent:
    tokens = nlp(sent)
    for pos in pos_dict:
      for i in range(len(tokens)):
        if tokens[i].tag_ == pos and re.findall('\W',tokens[i].text) == []:
          hop = 1
          cond = True
          while hop <= maxhops:
            if i+hop < len(tokens):
              if tokens[i+hop].tag_ == pos and re.findall('\W',tokens[i+hop].text) == [] and tokens[i+hop].text != tokens[i].text and tokens[i+hop].text != "app":
                bigram = "_".join([tokens[i].text, tokens[i+hop].text])
                hop = maxhops
                output.append(bigram)
            hop+=1
    length += len(tokens)
  return output

def make_list(series):
  output = []
  for idx in series.index:
    for pairs in series.loc[idx]:
      output.append((idx, pairs))
  return pd.DataFrame(output).rename({0:'origin_index',1:'keywords'}, axis = 1)

def make_pair_list(dataframe, pos_dict = pos_dict, pair_dict = pair_dict, maxhops = 3):
  dataframe = dataframe.apply(lambda x: x.lower())
  phrase = dataframe.apply(lambda x: find_phrase(x, pair_dict, maxhops))
  pairs = dataframe.apply(lambda x: find_pos_pairs(x, pos_dict, maxhops))
  phrase_list = make_list(phrase)
  pairs_list = make_list(pairs)
  return pd.concat([phrase_list, pairs_list])


# @title Final Keywords Result Merging helper functions
#Dropping duplicate keywords
def dropping_duplicate_keys(df, keys_df):  
  keyword_dict = pd.DataFrame()
  for i in df.index.drop_duplicates():
    indexed_df = keys_df.loc[i:i].drop_duplicates(subset=['keywords'])
    keyword_dict = pd.concat([keyword_dict, indexed_df])
  return keyword_dict

#Combining two results
def joining_keywords(origin_df, keys_df1, keys_df2):
  # joining into one
  df1 = keys_df1.copy()
  df1.reset_index(level = 'origin_index', inplace = True)
  df1 = pd.concat([df1, keys_df2]).sort_values(by = 'origin_index')
  df1.index = df1.origin_index
  df1.drop(['origin_index'], axis = 1, inplace = True)

  # dropping dupliactes
  df1 = dropping_duplicate_keys(origin_df, df1)

  # Perform an aggregation count of the keywords
  final_keys = pd.DataFrame(df1.keywords.value_counts())
  final_keys.reset_index(inplace = True)
  final_keys.rename({"keywords":"frequency","index":"keywords"}, axis = 1, inplace = True)

  # Count the polarity score
  """
  Adjusted score will be the frequency of the keyword * polarity of the keyword
  Absolute score simply take the magnitude only of the adjusted score, for sorting purpose
  """
  final_keys['polarity score'] = final_keys['keywords'].apply(lambda x: polarity_scoring(" ".join(x.split('_'))))
  final_keys['adjusted score'] = final_keys['frequency'] * final_keys['polarity score']
  final_keys['absolute score'] = final_keys['adjusted score'].abs()
  final_keys = final_keys.sort_values(by = ['frequency'], ascending = False)

  return final_keys

#Dropping "similar" keywords
def find_noun(keyword):
  keyword = TextBlob(' '.join(keyword.split('_'))).tags
  nouns = [word[0] for word in keyword if word[1] == 'NN']
  return nouns

def drop_same_noun(dataframe):
  nouns = []
  drop = []
  for idx in range(len(dataframe)):
    noun = find_noun(dataframe.loc[idx, 'keywords'])
    pol = dataframe.loc[idx, 'polarity score'] > 0
    if noun != []:
      if [noun, pol] in nouns:
        drop.append(idx)
      else:
        nouns.append([noun, pol])
  return dataframe.drop(drop)

def unmeaningful_keys(appName):
  appName = appName.lower()
  appName = spacy_lemmatize(appName, True)
  drop = [f'terima_kasih_{appName}', 'terima_kasih', f'kasih_{appName}', f'{appName}', "game_ever"]
  return drop

# @title Other helper functions

#Function to count the polarity scoring for sorting purpose
def polarity_scoring(sentence):
  return TextBlob(str(sentence)).sentiment.polarity

def polarity_scoring_for_review(sentence):
  return np.sum([polarity_scoring(sent) for sent in sent_tokenize(sentence)])

#Function to clean the app description
def descClean(appDesc):
  appDesc = re.sub('[\r\n*]', '.', appDesc)
  appDesc = re.sub(r'[\?\.\!]+(?=[\?\.\!])',' ', appDesc)
  return appDesc

def remove_puntuations(text):
    for punct in string.punctuation:
        text = text.replace(punct, '')
    return text

#helper functions to sort the reviews based on certain keywords
def subset_checker_2(review, keyword):
  """ This function is to check whether a keyword is in a sentence in a review """
  review = spacy_lemmatize(review, False)
  tokenized_review = sent_tokenize(review)
  set_keyword = set(keyword.split('_'))
  set_keyword.discard('and')
  set_keyword.discard('to')
  for sent in tokenized_review:
    sent = remove_puntuations(sent.lower())
    set_review = set(sent.split(' '))
    if set_keyword.issubset(set_review):
      return True
  return False
#@title get word count score
def get_word_count(sentence):
  word_count = len(sentence)
  word_count_log2 = math.log(word_count, 2)
  return word_count_log2

def sorting_by_keyword_with_score(df, keyword):
  truth_val = df.review.apply(lambda x: subset_checker_2(x, keyword))
  df_copy = df[truth_val].copy()
  df_copy.loc[:,'keyword'] = " ".join(keyword.split('_'))
  df_copy.loc[:,'polarity score'] = df_copy['review'].apply(polarity_scoring_for_review)
  df_copy.loc[:,'absolute score'] = df_copy['polarity score'].abs()
  df_copy.loc[:,'word count score'] = df_copy['review'].apply(get_word_count)
  df_copy = df_copy[['keyword', 'review', 'word count score', 'polarity score', 'rating']].sort_values(by='word count score', ascending = False)
  df_copy = df_copy.loc[:,['keyword','review', 'polarity score', 'rating']]

  drop_index = []
  if len(df_copy) < 3:
    for index in df_copy.index.unique():
        drop_index.append(index)

  df_copy = df_copy.drop(drop_index)

  if len(df_copy) == 0:
    average_sentiment = review_count = average_rating = 'dropped'
  else:
    average_sentiment = df_copy['polarity score'].mean()
    review_count = len(df_copy)
    average_rating = df_copy.rating.mean()
  
  df_copy = df_copy.loc[:,['keyword','review', 'polarity score']]

  return df_copy.head(5), average_sentiment, review_count, average_rating

def reviews_summary_score(subsetted_df):
  print("The average rating: "+str(subsetted_df.rating.mean()))
  print("The average score: " +str(subsetted_df['polarity score'].mean()))
  return subsetted_df.rating.mean(), subsetted_df['polarity score'].mean()

#############################################################################################################
#@title Combine all function to extract keywords
def extract_keywords(dataframe, appDesc, appTitle, minNGrams = [2,2], maxNGrams = [3,3], nKeywords = [3,10], windowSize = [5,5], removeNotFromStopWords = True, isConcatenated = True, pos_dict = pos_dict, pair_dict = pair_dict, maxhops = 3):
  # YAKE for app reviews
  review_yake = YAKEKeywords(dataframe['review'], minNGrams[0], maxNGrams[0], nKeywords[0], windowSize[0], removeNotFromStopWords)
  review_yake['keywords'] = review_yake['keywords'].apply(lambda x: keyword_lemmatize(x, isConcatenated))

  # Hops Phrase Finder for app reviews
  review_hop = make_pair_list(dataframe[dataframe.lang == 'en']['review'])
  review_hop['keywords'] = review_hop['keywords'].apply(lambda x: keyword_lemmatize(x, isConcatenated))

  # Merging the Keywords
  review_keys = joining_keywords(dataframe, review_yake, review_hop)
  review_keys.reset_index(drop = True, inplace = True)
  review_keys = drop_same_noun(review_keys) #dropping similar keywords

  review_keys = review_keys.sort_values(by = ['frequency', 'absolute score'], ascending = False)

  # Cleaning the app description
  appDesc = descClean(appDesc)

  # YAKE for app description
  desc_yake = YAKEKeywords(sent_tokenize(appDesc), minNGrams[1], maxNGrams[1], nKeywords[1], windowSize[1])
  desc_yake['keywords'] = desc_yake['keywords'].apply(lambda x: keyword_lemmatize(x, isConcatenated))

  # Hops Phrase Finder for app description
  desc_hop = make_pair_list(pd.Series(sent_tokenize(appDesc)))
  desc_hop['keywords'] = desc_hop['keywords'].apply(lambda x: keyword_lemmatize(x, isConcatenated))

  # Merging the Keywords
  desc_keys = joining_keywords(pd.Series(sent_tokenize(appDesc)), desc_yake, desc_hop)
  desc_keys.reset_index(drop = True, inplace = True)
  desc_keys = drop_same_noun(desc_keys)

  output = pd.merge(review_keys, desc_keys, how = "inner", on = ['keywords']).sort_values(by = ['frequency_x'], ascending = False)
  output.drop(output.columns[-4:], axis = 1, inplace = True)
  output.rename({"frequency_x":"frequency", "polarity score_x" : "polarity score", "adjusted score_x" : "adjusted score", "absolute score_x":"absolute score"}, axis = 1, inplace = True)
  output = drop_same_noun(output)
  output.reset_index(inplace = True)
  output = pd.merge(output, review_keys, how = "outer").drop(['index'], axis = 1)
  # output = output[output['frequency'] >= 5]
  drop_keys = unmeaningful_keys(appTitle)
  output = output[~output['keywords'].isin(drop_keys)]
  if len(output[output['frequency'] >= 5]) < 10:
    return output.head(10)
  else:
    return output[output['frequency'] >= 5].head(10)
  # return pd.merge(output, review_keys, how = "outer").drop(['index'], axis = 1).head(10)

def get_top_extracted_keywords(df, appDesc, appTitle):
  keywords = extract_keywords(df, appDesc, appTitle)
  reviews_count = len(df)
  min_freq = 0
  if reviews_count < 500:
    min_freq = 3
  elif 500 <= reviews_count < 2500:
    min_freq = 5
  elif 2500 <= reviews_count < 5000:
    min_freq = 8
  elif reviews_count > 5000:
    min_freq = 10

  keywords = keywords[keywords.frequency>min_freq].reset_index(drop=True).head(10)
  return keywords

def check_min_rows(final_df):
  drop_index = []
  for value in final_df.keyword.unique():
    df_check = final_df[final_df['keyword']==value]
    if len(df_check) < 3:
      for index in df_check.index.unique():
        drop_index.append(index)

  final_df = final_df.drop(drop_index)
  return final_df.reset_index(drop=True)

def get_topic_df(df, appDesc, appTitle):
  result = pd.DataFrame({"keyword":[], "review":[], "polarity score":[]})
  avg_rating_list = []
  avg_sentiment_list = [] 
  review_count_list = []
  top_5_reviews = pd.DataFrame()
  
  keywords = get_top_extracted_keywords(df, appDesc, appTitle)

  for keyword in keywords.keywords:
    try:
      top_5_reviews, average_sentiment, review_count, average_rating = sorting_by_keyword_with_score(df, keyword)
      if avg_rating_list != 'dropped':
        avg_rating_list.append(average_rating)
      if avg_sentiment_list != 'dropped':
        avg_sentiment_list.append(average_sentiment)
      if review_count_list != 'dropped':
        review_count_list.append(review_count)
    except:
      print(keyword,'Error!')
    result = pd.concat([result, top_5_reviews], axis = 0)
    

  arr_index = pd.MultiIndex.from_frame(result[['keyword', 'review']])
  final_df = pd.DataFrame(result['polarity score'].values, index = arr_index)
  final_df = final_df.reset_index()

  final_df = check_min_rows(final_df)

  return final_df, avg_rating_list, avg_sentiment_list, review_count_list

def deEmojify(text):
    '''
    Remove emoji from review data
    '''
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642" 
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
        "]+", flags = re.UNICODE)
    text=regrex_pattern.sub(r'',text)
    text=text.replace('\n',' ')
    text=re.sub(' +', ' ', text)

    return text

def remove_emoji(dataframe):
    '''
    Preprocessing 2 : Remove emoji from review data
    '''
    #print("Please wait, currently we're doing second preprocessing for your review data!")
    dataframe=dataframe.copy()
    dataframe['review']=dataframe['review'].apply(deEmojify)           #removing emoji
    return dataframe

def get_range_2_topic(review, word_len, top1, top2):
  
  word_len_min = word_len-365
  word_len_max = word_len
  threshold_min_f = 0
  threshold_max_f = 365
  for i in range(int(round(word_len/10,2))):
    threshold_max = word_len-10*i
    threshold_min = word_len_min-10*i
    if (threshold_max) > word_len_min:
      if top1 in review[threshold_min:threshold_max] or top2 in review[threshold_min:threshold_max]:
        threshold_min_f = threshold_min
        threshold_max_f = threshold_max

  for i in range(int(round(word_len/10,2))):
    threshold_max = word_len-10*i
    threshold_min = word_len_min-10*i
    if (threshold_max) > word_len_min:
      if top1 in review[threshold_min:threshold_max] and top2 in review[threshold_min:threshold_max]:
        threshold_min_f = threshold_min
        threshold_max_f = threshold_max
    else:
      if top1 in review[threshold_min:threshold_max] or top2 in review[threshold_min:threshold_max]:
        threshold_min_f = threshold_min
        threshold_max_f = threshold_max
      elif top1 in review[threshold_min:threshold_max] and top2 in review[threshold_min:threshold_max]:
        threshold_min_f = threshold_min
        threshold_max_f = threshold_max

  if threshold_max_f < 0 or threshold_min_f < 10:        
    threshold_min_f = 0
    threshold_max_f = 365 

  return threshold_min_f, threshold_max_f

def get_range_3_topic(review, word_len, top1, top2, top3):
  word_len_min = word_len-365
  word_len_max = word_len
  threshold_min_f = 0
  threshold_min = 0
  threshold_max_f = 365
  threshold_max = 365
  for i in range(int(round(word_len/10,2))):
    threshold_max = word_len-10*i
    threshold_min = word_len_min-10*i
    if (threshold_max) > word_len_min:
      if top1 in review[threshold_min:threshold_max] or top2 in review[threshold_min:threshold_max] or top3 in review[threshold_min:threshold_max]:
        threshold_min_f = threshold_min
        threshold_max_f = threshold_max

  for i in range(int(round(word_len/10,2))):
    threshold_max = word_len-10*i
    threshold_min = word_len_min-10*i
    if (threshold_max) > word_len_min:
      if top1 in review[threshold_min:threshold_max] and top2 in review[threshold_min:threshold_max] and top3 in review[threshold_min:threshold_max]:
        threshold_min_f = threshold_min
        threshold_max_f = threshold_max      
    else:
      if top1 in review[threshold_min:threshold_max] or top2 in review[threshold_min:threshold_max] or top3 in review[threshold_min:threshold_max]:
        threshold_min_f = threshold_min
        threshold_max_f = threshold_max
      elif top1 in review[threshold_min:threshold_max] and top2 in review[threshold_min:threshold_max] and top3 in review[threshold_min:threshold_max]:
        threshold_min_f = 365
        threshold_max_f = 0

  if threshold_max_f < 0 or threshold_min_f < 10:        
    threshold_min_f = 0
    threshold_max_f = 365 

  return threshold_min, threshold_max

def tag_topic(final_df):
  color_green = '#ddedde'

  for topic in final_df.keyword.unique():
    topic_splits = topic.split()
    df_split = final_df[final_df['keyword']==topic]
    threshold_min = 0
    threshold_max = 0

    for review in df_split.review:
      idx = df_split[df_split['review']==review].index

      word_len = len(review)
      len_topic = len(topic_splits)
      if len_topic == 2:
        top1 = topic_splits[0]
        top2 = topic_splits[1]
        threshold_min, threshold_max =  get_range_2_topic(review, word_len, top1, top2)
      elif len_topic >= 3:
        top1 = topic_splits[0]
        top2 = topic_splits[1]
        top3 = topic_splits[2]
        threshold_min, threshold_max =  get_range_3_topic(review, word_len, top1, top2, top3)
      
      review = review[threshold_min:threshold_max]  

      word_counts = len(review)

      if threshold_max >= 365:
        review += "..."
      if threshold_min > 0:
        review = "..." + review
    
      for topic_split in topic_splits:
        review = review.replace(topic_split, "<font backcolor={0}><b> {1}</b></font>".format(color_green, topic_split))
        review = review.replace("<font backcolor={}><b> ".format(color_green), " <font backcolor={}><b>".format(color_green)) 
        review = review.replace("</b></font>  <font backcolor={}><b>".format(color_green), "</b></font><font backcolor={}><b> ".format(color_green))

      
      final_df.review[idx] = review
  
  final_df = remove_emoji(final_df)

  return final_df

def get_topic_table(df_split):
  ps = ParagraphStyle('title', fontSize=20, leading=22)

  list_reviews = ['', '', '', '', '']
  list_scores = ['', '', '', '', '']

  for i in range(len(df_split)):
      list_reviews[i] = df_split.review[i]
      list_scores[i] = df_split.polarity_score[i]


  rev_tab = [['Reviews', 'Polarity Score'],
            [Paragraph(list_reviews[0],ps), round(list_scores[0],2)],
            [Paragraph(list_reviews[1],ps), round(list_scores[1],2)],
            [Paragraph(list_reviews[2], ps), round(list_scores[2],2)],
            [Paragraph(list_reviews[3], ps), round(list_scores[3],2)],
            [Paragraph(list_reviews[4], ps), round(list_scores[4],2)]]

  review_table = Table(rev_tab, [1160, 120], rowHeights=(30, 90, 90, 90, 90, 90))

  # Set font style
  font = TableStyle([
    ('FONTNAME', (0,0), (-1,5), 'Helvetica-Bold'),
    ('TEXTCOLOR',(0,0),(-1,0),colors.whitesmoke),
    ('FONTSIZE', (0,0), (1,1), 15),
    ('FONTSIZE', (0,1), (1,1), 21),
    ('FONTSIZE', (0,2), (1,2), 21),
    ('FONTSIZE', (0,3), (1,3), 21),
    ('FONTSIZE', (0,4), (1,4), 21),
    ('FONTSIZE', (0,5), (1,5), 21),
    #('FONTSIZE', (0,1), (-1,-1), 15),
  ])

  review_table.setStyle(font)

  # Set Table padding
  padding = TableStyle([
    ('ALIGN',(0,0),(-1,0),'CENTER'),
    ('ALIGN',(1,1),(-1,-1),'CENTER'),
    ('VALIGN',(0,0),(-1,-1),'MIDDLE'),
    ('ALIGN',(0,1),(-1,0),'LEFT'),
    ('BOTTOMPADDING', (0,0), (-1,0), 15),
    ('BOTTOMPADDING', (0,1), (-1,-1), 8),
    ('TOPPADDING', (0,0), (-1,0), 12),
    ('TOPPADDING', (0,1), (-1,-1), 8),
    ('LEADING', (0,1), (1,1), 23),
    ('LEADING', (0,2), (1,2), 23),
    ('LEADING', (0,3), (1,3), 23),
    ('LEADING', (0,4), (1,4), 23),
    ('LEADING', (0,5), (1,5), 23),

    ('BACKGROUND',(0,1),(-1,-1),colors.burlywood),
  ])
  
  review_table.setStyle(padding)

  # 3) Alternate backgroud color
  cx = colors.white
  c0 = colors.black
  c1 = colors.HexColor('#9f8159')
  c2 = colors.HexColor('#b59a76')
  c3 = colors.HexColor('#c2aa8a')
  c4 = colors.HexColor('#c9b69d')
  c5 = colors.HexColor('#dcd3c4')

  list_color = [c0, cx, cx, cx, cx, cx]

  row = len(df_split)
  if row == 0:
    list_color = [c0, cx, cx, cx, cx, cx]
  elif row == 1:
    list_color = [c0, c1, cx, cx, cx, cx]
  elif row == 2:
    list_color = [c0, c1, c2, cx, cx, cx]
  elif row == 3:
    list_color = [c0, c1, c2, c3, cx, cx]
  elif row == 4:
    list_color = [c0, c1, c2, c3, c4, cx]
  elif row == 5:
    list_color = [c0, c1, c2, c3, c4, c5]

  color = TableStyle([
      ('BACKGROUND', (0,0), (3,0), list_color[0]),
      ('BACKGROUND', (0,1),(2,1), list_color[1]),
      ('BACKGROUND', (0,2),(2,2), list_color[2]),
      ('BACKGROUND', (0,3),(2,3), list_color[3]),
      ('BACKGROUND', (0,4),(2,4), list_color[4]),
      ('BACKGROUND', (0,5),(2,5), list_color[5]),
      ]
    )

  review_table.setStyle(color)

  return review_table
